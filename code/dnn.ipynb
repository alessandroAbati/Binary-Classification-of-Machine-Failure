{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>L50096</td>\n",
       "      <td>L</td>\n",
       "      <td>300.6</td>\n",
       "      <td>309.6</td>\n",
       "      <td>1596</td>\n",
       "      <td>36.1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M20343</td>\n",
       "      <td>M</td>\n",
       "      <td>302.6</td>\n",
       "      <td>312.1</td>\n",
       "      <td>1759</td>\n",
       "      <td>29.1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>L49454</td>\n",
       "      <td>L</td>\n",
       "      <td>299.3</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1805</td>\n",
       "      <td>26.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>L53355</td>\n",
       "      <td>L</td>\n",
       "      <td>301.0</td>\n",
       "      <td>310.9</td>\n",
       "      <td>1524</td>\n",
       "      <td>44.3</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M24050</td>\n",
       "      <td>M</td>\n",
       "      <td>298.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1641</td>\n",
       "      <td>35.4</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0   0     L50096    L                300.6                    309.6   \n",
       "1   1     M20343    M                302.6                    312.1   \n",
       "2   2     L49454    L                299.3                    308.5   \n",
       "3   3     L53355    L                301.0                    310.9   \n",
       "4   4     M24050    M                298.0                    309.0   \n",
       "\n",
       "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Machine failure  TWF  \\\n",
       "0                    1596         36.1              140              0.0    0   \n",
       "1                    1759         29.1              200              0.0    0   \n",
       "2                    1805         26.5               25              0.0    0   \n",
       "3                    1524         44.3              197              0.0    0   \n",
       "4                    1641         35.4               34              0.0    0   \n",
       "\n",
       "   HDF  PWF  OSF  RNF  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "2    0    0    0    0  \n",
       "3    0    0    0    0  \n",
       "4    0    0    0    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "dataset = pd.concat([train,test])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DUPLICATES\n",
    "train.drop_duplicates(subset=train.columns.difference(['id']),inplace=True)\n",
    "test.drop_duplicates(subset=test.columns.difference(['id']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING VALUES\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train Product ID.unique = 9976, test Product ID.unique = 9909\n",
      " train Type.unique = 3, test Type.unique = 3\n"
     ]
    }
   ],
   "source": [
    "#Searching for non-ordinal categorical features\n",
    "categorical_columns = train.select_dtypes(include=['object']).columns.values\n",
    "#Calculating unique values of categorical features\n",
    "for col in categorical_columns:\n",
    "    print(f\" train {col}.unique = {len(train[col].unique())}, test {col}.unique = {len(test[col].unique())}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Product Id has too many unique values we cannot use one-hot encoding for this categorical feature because that will increase the dimesion of the feature space too much, resulting in slowing down the training time. Because of that we will use one-hot encoding for the Type feature and Frequency encoding for the ProductID feature (Some suggests to use Target enconding but I think that leads to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODING of Type column\n",
    "for df in [train, test]:\n",
    "    for value in df.Type.unique():\n",
    "        df[f'Type{value}'] = 0\n",
    "        df.loc[df.Type == f'{value}', f'Type{value}'] = 1\n",
    "    df.drop(columns=['Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency ENCODING of Product ID column (It is a way to utilize the frequency of the categories as labels)\n",
    "for df in [train, test]:\n",
    "    df['EncodedProductID'] = df.groupby(by=['Product ID'])['Product ID'].transform('count')\n",
    "    df.drop(columns=['Product ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    # Create a new feature by subtracting 'Air temperature' from 'Process temperature'\n",
    "    # df['Temperature difference [K]'] = df['Process temperature [K]'] - df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by divided 'Air temperature' from 'Process temperature'\n",
    "    df[\"Temperature ratio\"] = df['Process temperature [K]'] / df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' and 'Rotational speed' (POWER)\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    # Create a new feature by multiplying 'Torque' by 'Tool wear'\n",
    "    df['Torque * Tool wear'] = df['Torque [Nm]'] * df['Tool wear [min]']\n",
    "\n",
    "    # Create a new feature by adding 'Air temperature' and 'Process temperature'\n",
    "    # df['Temperature sum [K]'] = df['Air temperature [K]'] + df['Process temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' by 'Rotational speed'\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    df['TotalFailures'] = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].sum(axis=1)\n",
    "\n",
    "    df.drop(['RNF'], axis =1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(n_features,)))\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units1\", min_value=8, max_value=512, step=16), activation='relu'))\n",
    "        if hp.Boolean(\"BatchNormalization1\", default=True):\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units2\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units3\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units4\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units5\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units6\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['AUC'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, validation_data=None, **kwargs):\n",
    "        return model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            batch_size = hp.Int(\"batch_size\", min_value=8, max_value=512, step=8),\n",
    "            validation_data=validation_data,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(columns=['id', 'Machine failure']).reset_index(drop=True)\n",
    "train_y = train['Machine failure'].reset_index(drop=True)\n",
    "n_features = len(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and test\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.33)\n",
    "\n",
    "#using the actual test set\n",
    "test_X = test.drop(columns=['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135295, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "sc = StandardScaler() # MinMaxScaler or StandardScaler\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and val\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighting the unbalanced target\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(train_y),\n",
    "                                                 y=train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 144       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8)                32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,233\n",
      "Trainable params: 4,961\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=2,\n",
    "    num_initial_points = 1,\n",
    "    overwrite=True,\n",
    "    directory=\"../hyperOptModelsHistory\",\n",
    "    project_name=\"BinaryClassificationofMachineFailure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "232               |232               |units1\n",
      "False             |False             |BatchNormalization1\n",
      "384               |384               |units2\n",
      "352               |352               |units3\n",
      "64                |64                |units4\n",
      "352               |352               |units5\n",
      "448               |448               |units6\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 232)               4176      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 384)               89472     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 384)              1536      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 352)               135520    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 352)              1408      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                22592     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 352)               22880     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 352)              1408      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 448)               158144    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 449       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 437,841\n",
      "Trainable params: 435,537\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11331/11331 [==============================] - 83s 7ms/step - loss: 0.4314 - auc: 0.9087 - val_loss: 0.1488 - val_auc: 0.9180\n",
      "Epoch 2/10\n",
      "11331/11331 [==============================] - 82s 7ms/step - loss: 0.4021 - auc: 0.9160 - val_loss: 0.2615 - val_auc: 0.7728\n",
      "Epoch 3/10\n",
      "11331/11331 [==============================] - 73s 6ms/step - loss: 0.3570 - auc: 0.9197 - val_loss: 0.0433 - val_auc: 0.9295\n",
      "Epoch 4/10\n",
      " 8753/11331 [======================>.......] - ETA: 18s - loss: 0.3091 - auc: 0.9283"
     ]
    }
   ],
   "source": [
    "tuner.search(train_X, train_y, epochs=10, validation_data=(val_X, val_y), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models.\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(input_shape=(None, 28, 28))\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "# Build the model with the best hp.\n",
    "model = build_model(best_hps[0])\n",
    "# Fit with the entire dataset.\n",
    "train_X = np.concatenate((train_X, val_X))\n",
    "train_y = np.concatenate((train_y, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, batch_size=512, epochs=1000, class_weight=class_weights, callbacks=[tf.keras.callbacks.EarlyStopping(patience=100)], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "y_pred = model.predict(val_X)\n",
    "fpr, tpr, _ = roc_curve(val_y,  y_pred)\n",
    "auc = roc_auc_score(val_y, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    \"Machine failure\" : np.squeeze(y_pred)\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output['Machine failure']>0.5].sort_values(by=['Machine failure'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
