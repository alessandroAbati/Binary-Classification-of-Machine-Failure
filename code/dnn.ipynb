{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "dataset = pd.concat([train,test])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DUPLICATES\n",
    "train.drop_duplicates(subset=train.columns.difference(['id']),inplace=True)\n",
    "test.drop_duplicates(subset=test.columns.difference(['id']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING VALUES\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching for non-ordinal categorical features\n",
    "categorical_columns = train.select_dtypes(include=['object']).columns.values\n",
    "#Calculating unique values of categorical features\n",
    "for col in categorical_columns:\n",
    "    print(f\" train {col}.unique = {len(train[col].unique())}, test {col}.unique = {len(test[col].unique())}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Product Id has too many unique values we cannot use one-hot encoding for this categorical feature because that will increase the dimesion of the feature space too much, resulting in slowing down the training time. Because of that we will use one-hot encoding for the Type feature and Frequency encoding for the ProductID feature (Some suggests to use Target enconding but I think that leads to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODING of Type column\n",
    "for df in [train, test]:\n",
    "    for value in df.Type.unique():\n",
    "        df[f'Type{value}'] = 0\n",
    "        df.loc[df.Type == f'{value}', f'Type{value}'] = 1\n",
    "    df.drop(columns=['Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency ENCODING of Product ID column (It is a way to utilize the frequency of the categories as labels)\n",
    "for df in [train, test]:\n",
    "    df['EncodedProductID'] = df.groupby(by=['Product ID'])['Product ID'].transform('count')\n",
    "    df.drop(columns=['Product ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    # Create a new feature by subtracting 'Air temperature' from 'Process temperature'\n",
    "    # df['Temperature difference [K]'] = df['Process temperature [K]'] - df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by divided 'Air temperature' from 'Process temperature'\n",
    "    df[\"Temperature ratio\"] = df['Process temperature [K]'] / df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' and 'Rotational speed' (POWER)\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    # Create a new feature by multiplying 'Torque' by 'Tool wear'\n",
    "    df['Torque * Tool wear'] = df['Torque [Nm]'] * df['Tool wear [min]']\n",
    "\n",
    "    # Create a new feature by adding 'Air temperature' and 'Process temperature'\n",
    "    # df['Temperature sum [K]'] = df['Air temperature [K]'] + df['Process temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' by 'Rotational speed'\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    df['TotalFailures'] = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].sum(axis=1)\n",
    "\n",
    "    df.drop(['RNF'], axis =1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(n_features,)))\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"layer1\", min_value=8, max_value=512, step=16), activation='relu'))\n",
    "        if hp.Boolean(\"BatchNormalization1\", default=True):\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        for i in range(hp.Int('number-of-hidden-layers', 1, 3, default=1)):\n",
    "            model.add(tf.keras.layers.Dense(units=hp.Int(\"hidden-layer\"+str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
    "            if hp.Boolean(\"BatchNormalization\"+str(i), default=True):\n",
    "                model.add(tf.keras.layers.BatchNormalization())\n",
    "            if i > 1:\n",
    "                if hp.Boolean(\"Dropout\"+str(i-1), default=False):\n",
    "                    model.add(tf.keras.layers.Dropout(hp.Float('dropout-'+str(i-1), 0, 0.5, step=0.1, default=0.5)))\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"final-layer\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')), metrics=['AUC'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, validation_data=None, **kwargs):\n",
    "        return model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            batch_size = hp.Int(\"batch_size\", min_value=8, max_value=512, step=8),\n",
    "            validation_data=validation_data,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(columns=['id', 'Machine failure']).reset_index(drop=True)\n",
    "train_y = train['Machine failure'].reset_index(drop=True)\n",
    "n_features = len(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and test\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.33)\n",
    "\n",
    "#using the actual test set\n",
    "test_X = test.drop(columns=['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "sc = StandardScaler() # MinMaxScaler or StandardScaler\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and val\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighting the unbalanced target\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(train_y),\n",
    "                                                 y=train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=2,\n",
    "    num_initial_points = 1,\n",
    "    overwrite=True,\n",
    "    directory=\"../hyperOptModelsHistory\",\n",
    "    project_name=\"BinaryClassificationofMachineFailure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_X, train_y, epochs=300, validation_data=(val_X, val_y), class_weight=class_weights, callbacks=[tf.keras.callbacks.EarlyStopping(patience=30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models.\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(input_shape=(None, 28, 28))\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain the best model with the best hps\n",
    "hypermodel = MyHyperModel()\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "model = hypermodel.build(best_hp)\n",
    "# Fit with the entire dataset.\n",
    "train_X = np.concatenate((train_X, val_X))\n",
    "train_y = np.concatenate((train_y, val_y))\n",
    "history = hypermodel.fit(best_hp, model, train_X, train_y, epochs=1000, class_weight=class_weights, callbacks=[tf.keras.callbacks.EarlyStopping(patience=30)], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "y_pred = model.predict(val_X)\n",
    "fpr, tpr, _ = roc_curve(val_y,  y_pred)\n",
    "auc = roc_auc_score(val_y, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    \"Machine failure\" : np.squeeze(y_pred)\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output['Machine failure']>0.5].sort_values(by=['Machine failure'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
