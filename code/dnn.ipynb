{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Machine failure</th>\n",
       "      <th>TWF</th>\n",
       "      <th>HDF</th>\n",
       "      <th>PWF</th>\n",
       "      <th>OSF</th>\n",
       "      <th>RNF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>L50096</td>\n",
       "      <td>L</td>\n",
       "      <td>300.6</td>\n",
       "      <td>309.6</td>\n",
       "      <td>1596</td>\n",
       "      <td>36.1</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M20343</td>\n",
       "      <td>M</td>\n",
       "      <td>302.6</td>\n",
       "      <td>312.1</td>\n",
       "      <td>1759</td>\n",
       "      <td>29.1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>L49454</td>\n",
       "      <td>L</td>\n",
       "      <td>299.3</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1805</td>\n",
       "      <td>26.5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>L53355</td>\n",
       "      <td>L</td>\n",
       "      <td>301.0</td>\n",
       "      <td>310.9</td>\n",
       "      <td>1524</td>\n",
       "      <td>44.3</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M24050</td>\n",
       "      <td>M</td>\n",
       "      <td>298.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1641</td>\n",
       "      <td>35.4</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0   0     L50096    L                300.6                    309.6   \n",
       "1   1     M20343    M                302.6                    312.1   \n",
       "2   2     L49454    L                299.3                    308.5   \n",
       "3   3     L53355    L                301.0                    310.9   \n",
       "4   4     M24050    M                298.0                    309.0   \n",
       "\n",
       "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Machine failure  TWF  \\\n",
       "0                    1596         36.1              140              0.0    0   \n",
       "1                    1759         29.1              200              0.0    0   \n",
       "2                    1805         26.5               25              0.0    0   \n",
       "3                    1524         44.3              197              0.0    0   \n",
       "4                    1641         35.4               34              0.0    0   \n",
       "\n",
       "   HDF  PWF  OSF  RNF  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "2    0    0    0    0  \n",
       "3    0    0    0    0  \n",
       "4    0    0    0    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "dataset = pd.concat([train,test])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DUPLICATES\n",
    "train.drop_duplicates(subset=train.columns.difference(['id']),inplace=True)\n",
    "test.drop_duplicates(subset=test.columns.difference(['id']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING VALUES\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train Product ID.unique = 9976, test Product ID.unique = 9909\n",
      " train Type.unique = 3, test Type.unique = 3\n"
     ]
    }
   ],
   "source": [
    "#Searching for non-ordinal categorical features\n",
    "categorical_columns = train.select_dtypes(include=['object']).columns.values\n",
    "#Calculating unique values of categorical features\n",
    "for col in categorical_columns:\n",
    "    print(f\" train {col}.unique = {len(train[col].unique())}, test {col}.unique = {len(test[col].unique())}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Product Id has too many unique values we cannot use one-hot encoding for this categorical feature because that will increase the dimesion of the feature space too much, resulting in slowing down the training time. Because of that we will use one-hot encoding for the Type feature and Frequency encoding for the ProductID feature (Some suggests to use Target enconding but I think that leads to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE-HOT ENCODING of Type column\n",
    "for df in [train, test]:\n",
    "    for value in df.Type.unique():\n",
    "        df[f'Type{value}'] = 0\n",
    "        df.loc[df.Type == f'{value}', f'Type{value}'] = 1\n",
    "    df.drop(columns=['Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency ENCODING of Product ID column (It is a way to utilize the frequency of the categories as labels)\n",
    "for df in [train, test]:\n",
    "    df['EncodedProductID'] = df.groupby(by=['Product ID'])['Product ID'].transform('count')\n",
    "    df.drop(columns=['Product ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    # Create a new feature by subtracting 'Air temperature' from 'Process temperature'\n",
    "    # df['Temperature difference [K]'] = df['Process temperature [K]'] - df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by divided 'Air temperature' from 'Process temperature'\n",
    "    df[\"Temperature ratio\"] = df['Process temperature [K]'] / df['Air temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' and 'Rotational speed' (POWER)\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    # Create a new feature by multiplying 'Torque' by 'Tool wear'\n",
    "    df['Torque * Tool wear'] = df['Torque [Nm]'] * df['Tool wear [min]']\n",
    "\n",
    "    # Create a new feature by adding 'Air temperature' and 'Process temperature'\n",
    "    # df['Temperature sum [K]'] = df['Air temperature [K]'] + df['Process temperature [K]']\n",
    "    \n",
    "    # Create a new feature by multiplying 'Torque' by 'Rotational speed'\n",
    "    df['Torque * Rotational speed'] = df['Torque [Nm]'] * df['Rotational speed [rpm]']\n",
    "\n",
    "    df['TotalFailures'] = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].sum(axis=1)\n",
    "\n",
    "    df.drop(['RNF'], axis =1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(17,)))\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units1\", min_value=8, max_value=512, step=16), activation='relu'))\n",
    "        if hp.Boolean(\"BatchNormalization1\", default=True):\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units2\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units3\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units4\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units5\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int(\"units6\", min_value=32, max_value=512, step=32), activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['AUC'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, validation_data=None, **kwargs):\n",
    "        return model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            batch_size = hp.Int(\"batch_size\", min_value=8, max_value=512, step=8),\n",
    "            validation_data=validation_data,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(columns=['id', 'Machine failure']).reset_index(drop=True)\n",
    "train_y = train['Machine failure'].reset_index(drop=True)\n",
    "n_features = len(train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and test\n",
    "#train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.33)\n",
    "\n",
    "#using the actual test set\n",
    "test_X = test.drop(columns=['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135295, 17)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "sc = StandardScaler() # MinMaxScaler or StandardScaler\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train dataset into train and val\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighting the unbalanced target\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(train_y),\n",
    "                                                 y=train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 8)                 144       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8)                32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,233\n",
      "Trainable params: 4,961\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11331/11331 [==============================] - 51s 4ms/step - loss: 0.3444 - auc: 0.9029 - val_loss: 0.2802 - val_auc: 0.9374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x143746ee130>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "hypermodel = MyHyperModel()\n",
    "model = hypermodel.build_model(hp)\n",
    "hypermodel.fit(hp, model, train_X, train_y, epochs=1, validation_data=(val_X, val_y), class_weight=class_weights, callbacks=[tf.keras.callbacks.EarlyStopping(patience=15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner \u001b[39m=\u001b[39m keras_tuner\u001b[39m.\u001b[39;49mBayesianOptimization(\n\u001b[0;32m      2\u001b[0m     hypermodel\u001b[39m=\u001b[39;49mMyHyperModel(),\n\u001b[0;32m      3\u001b[0m     objective\u001b[39m=\u001b[39;49mkeras_tuner\u001b[39m.\u001b[39;49mObjective(\u001b[39m\"\u001b[39;49m\u001b[39mval_auc\u001b[39;49m\u001b[39m\"\u001b[39;49m, direction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m      4\u001b[0m     max_trials\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     num_initial_points \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m     directory\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../hyperOptModelsHistory\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     project_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBinaryClassificationofMachineFailure\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\tuners\\bayesian.py:393\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[1;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    366\u001b[0m     hypermodel\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    379\u001b[0m ):\n\u001b[0;32m    380\u001b[0m     oracle \u001b[39m=\u001b[39m BayesianOptimizationOracle(\n\u001b[0;32m    381\u001b[0m         objective\u001b[39m=\u001b[39mobjective,\n\u001b[0;32m    382\u001b[0m         max_trials\u001b[39m=\u001b[39mmax_trials,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m         max_consecutive_failed_trials\u001b[39m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[0;32m    392\u001b[0m     )\n\u001b[1;32m--> 393\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(oracle\u001b[39m=\u001b[39moracle, hypermodel\u001b[39m=\u001b[39mhypermodel, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:113\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m hypermodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mrun_trial \u001b[39mis\u001b[39;00m Tuner\u001b[39m.\u001b[39mrun_trial:\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`hypermodel` if the user defines the search space in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing a `HyperModel` instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    114\u001b[0m     oracle\u001b[39m=\u001b[39moracle,\n\u001b[0;32m    115\u001b[0m     hypermodel\u001b[39m=\u001b[39mhypermodel,\n\u001b[0;32m    116\u001b[0m     directory\u001b[39m=\u001b[39mdirectory,\n\u001b[0;32m    117\u001b[0m     project_name\u001b[39m=\u001b[39mproject_name,\n\u001b[0;32m    118\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[0;32m    119\u001b[0m     overwrite\u001b[39m=\u001b[39moverwrite,\n\u001b[0;32m    120\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_model_size \u001b[39m=\u001b[39m max_model_size\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:133\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreload()\n\u001b[0;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[39m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_populate_initial_space()\n\u001b[0;32m    135\u001b[0m \u001b[39m# Run in distributed mode.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m dist_utils\u001b[39m.\u001b[39mis_chief_oracle():\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Blocks forever.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:204\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[1;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_activate_all_conditions()\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:161\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_space()\n\u001b[0;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mbuild(hp)\n\u001b[0;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Update the recorded scopes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:115\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtunable:\n\u001b[0;32m    112\u001b[0m     \u001b[39m# Copy `HyperParameters` object so that new entries are not added\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[39m# to the search space.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     hp \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aless\\miniconda3\\envs\\tf2.1-gpu\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:108\u001b[0m, in \u001b[0;36mHyperModel.build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, hp):\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Builds a model.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m        A model instance.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=2,\n",
    "    num_initial_points = 1,\n",
    "    overwrite=True,\n",
    "    directory=\"../hyperOptModelsHistory\",\n",
    "    project_name=\"BinaryClassificationofMachineFailure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROVA\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "from keras import layers\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"tune_hypermodel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner\u001b[39m.\u001b[39msearch(train_X, train_y, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.search(train_X, train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_X, train_y, epochs=10, validation_data=(val_X, val_y), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models.\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# Needed for `Sequential` without specified `input_shape`.\n",
    "best_model.build(input_shape=(None, 28, 28))\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "# Build the model with the best hp.\n",
    "model = build_model(best_hps[0])\n",
    "# Fit with the entire dataset.\n",
    "train_X = np.concatenate((train_X, val_X))\n",
    "train_y = np.concatenate((train_y, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, batch_size=512, epochs=1000, class_weight=class_weights, callbacks=[tf.keras.callbacks.EarlyStopping(patience=100)], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "y_pred = model.predict(val_X)\n",
    "fpr, tpr, _ = roc_curve(val_y,  y_pred)\n",
    "auc = roc_auc_score(val_y, y_pred)\n",
    "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    \"Machine failure\" : np.squeeze(y_pred)\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output['Machine failure']>0.5].sort_values(by=['Machine failure'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
